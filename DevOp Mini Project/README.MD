This project demonstrates an end-to-end data engineering pipeline built with DevOps best practices.
It ingests raw JSON data, validates data quality, applies SQL-based transformations, and orchestrates execution using Apache Airflow, all running in Dockerized services with CI/CD integration.

The project is designed to showcase data engineering + DevOps skills suitable for real-world production pipelines.

ğŸ— Architecture Overview
Raw JSON Files
      â†“
Python Ingestion
      â†“
PostgreSQL (raw schema)
      â†“
SQL Validation (quality checks)
      â†“
SQL Transformation
      â†“
PostgreSQL (processed schema)
      â†“
Apache Airflow (orchestration)
      â†“
CI/CD Testing & Monitoring

ğŸ§° Tech Stack

Programming: Python, SQL

Database: PostgreSQL

Orchestration: Apache Airflow

Containerization: Docker, Docker Compose

Data Processing: Pandas, SQLAlchemy

CI/CD: GitLab CI

Testing: Pytest (unit, SQL, DAG integrity)

ğŸ“‚ Project Structure
DevOp Mini Project/
â”‚
â”œâ”€â”€ ingestion/
â”‚   â””â”€â”€ ingestion_raw_data.py
â”‚
â”œâ”€â”€ data_processing/
â”‚   â”œâ”€â”€ data_validation/
â”‚   â”‚   â”œâ”€â”€ validation.sql
â”‚   â”‚   â””â”€â”€ validation.py
â”‚   â”‚
â”‚   â””â”€â”€ data_transformation/
â”‚       â”œâ”€â”€ transformation.sql
â”‚       â””â”€â”€ transformation.py
â”‚
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ data_pipeline_dag.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ sql/
â”‚   â””â”€â”€ dag/
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitlab-ci.yml

ğŸ”„ Pipeline Steps
1ï¸âƒ£ Data Ingestion

Reads multiple newline-delimited JSON files from a local folder

Loads raw data into PostgreSQL (raw schema)

Handles large files using batch inserts

2ï¸âƒ£ Data Validation

Executes SQL-based quality checks:

Null value detection

Duplicate record detection

Row count validation

Pipeline fails automatically if validation does not pass

3ï¸âƒ£ Data Transformation

Cleans validated data

Generates analytics-ready tables

Adds derived metrics (e.g. magnitude from accelerometer readings)

Writes results into a processed schema

4ï¸âƒ£ Orchestration (Airflow)

Tasks:

Ingestion

Validation

Transformation

Enforces task dependencies

Supports retries and logging

Visualized and monitored via Airflow UI

 Running the Project (Local)
Prerequisites

Docker Desktop

Python 3.10+

Git

Start Services
docker compose up -d

Access Airflow UI
http://localhost:8080


Default credentials:

Username: airflow
Password: airflow

// Testing

This project includes automated tests:

Unit tests for ingestion, validation, and transformation logic

SQL tests to verify table creation and row counts

DAG integrity tests to ensure Airflow DAGs load correctly

Run tests locally:

pytest

// CI/CD (GitLab)

GitLab CI pipeline automatically:

Installs dependencies

Runs all tests

Validates DAG integrity

Ensures pipeline reliability before deployment

// Monitoring & Alerting

Task-level monitoring via Airflow UI

Failure alerts via task status and logs

Pipeline stops automatically on validation failure

// Key Learnings & Best Practices

Data quality checks are enforced before transformation

SQL used for heavy data logic to improve performance

Clear separation of raw, staging, and processed layers

Containerized environment ensures reproducibility

CI/CD ensures safe and reliable pipeline changes

// Future Improvements

Add Great Expectations for advanced data validation

Implement incremental ingestion

Deploy to cloud (AWS/GCP)

ğŸ‘¤ Author

Phu Nguyen Xuan
DevOps / Data Engineering Enthusiast